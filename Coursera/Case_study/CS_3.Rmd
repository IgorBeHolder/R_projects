---
title: "Coursera Capstone project"
author: "Igr Sorochan"
date: "`r Sys.Date()`"
output:
  github_document:
    toc: yes
    toc_depth: 4
  pdf_document:
    toc: yes
    toc_depth: '4'
  chunk_output_type: inline
  html_document:
    toc: yes
    toc_depth: 4
    df_print: paged
---

+----------------------------------------------------------------------------------------------------------------------------------------------------------------+-----------+-----------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------+
| ![](https://companieslogo.com/img/orig/COUR_BIG-e3284ace.png?t=1631031088){width="127"}                                                                        |           | ![](https://d3njjcbhbojbot.cloudfront.net/api/utilities/v1/imageproxy/http://coursera-university-assets.s3.amazonaws.com/fa/79e521abf14610b4fec9d677901916/0.png?auto=format%252Ccompress&dpr=2&w=&h=45){style="vertical-align: middle" width="90" height="29"} |
+----------------------------------------------------------------------------------------------------------------------------------------------------------------+-----------+-----------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------+
| Coursera is the global online learning platform that offers anyone, anywhere access to online courses and degrees from world-class universities and companies. |           | Google is an American multinational corporation specializing in internet-related services and products.                                                                                                                                                         |
+----------------------------------------------------------------------------------------------------------------------------------------------------------------+-----------+-----------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------+

# Case Study 1 Case Study: How Does a Bike-Share navigate Speedy Success?

## 1. ASK

### Scenario

I'm a junior data analyst working in the marketing analyst team at Cyclistic, a bike-share company in Chicago.\

Lily Moreno, the director of marketing, believes the company's future success depends on maximizing the number of annual memberships.
Therefore, my team wants to understand\
\
**How casual riders and annual members use Cyclistic bikes differently?**\

### Settings

**About the company**

Cyclistic is bike share system across Chicago and Evanston.
Cyclistic provides residents and visitors with a convenient, fun and affordable transportation option for getting around and exploring Chicago.

Cyclistic, like other bike share systems, consists of a fleet of specially-designed, sturdy and durable bikes that are locked into a network of docking stations throughout the region.
The bikes can be unlocked from one station and returned to any other station in the system.
People use bike share to explore Chicago, commute to work or school, run errands, get to appointments or social engagements, and more.

Cyclistic is available for use 24 hours/day, 7 days/week, 365 days/year, and riders have access to all bikes and stations across the system.

Until now, Cyclistic's marketing strategy relied on building general awareness and appealing to broad consumer segments.
One approach that helped make these things possible was the flexibility of its pricing plans: single-ride passes, full-day passes, and annual memberships.
Customers who purchase single-ride or full-day passes are referred to as casual riders.
Customers who purchase annual memberships are Cyclistic members.

Cyclistic's finance analysts have concluded that annual members are much more profitable than casual riders.
Although the pricing flexibility helps Cyclistic attract more customers, Moreno believes that **maximizing the number of annual members** will be **key to future growth**.
Rather than creating a marketing campaign that targets all-new customers, Moreno believes there is a very good chance **to convert casual riders into members**.
She notes that casual riders are already aware of the Cyclistic program and have chosen Cyclistic for their mobility needs.
Moreno has set a clear goal: **Design marketing strategies aimed at converting casual riders into annual members**.
In order to do that, however, the marketing analyst team needs to better understand how annual members and casual riders differ, why casual riders would buy a membership, and how digital media could affect their marketing tactics.
Moreno and her team are interested in analyzing the Cyclistic historical bike trip data to identify trends.

### **Questions my team has to answer:**

1.  How casual riders and annual members use Cyclistic bikes differently?

2.  Why would casual riders buy Cyclistic annual memberships?

3.  How can Cyclistic use digital media to influence casual riders to become members?

### Project stakeholders

**Primary stakeholders:**

-   Cyclistic executive team

-   Lily Moreno, the director of marketing

**Secondary stakeholders:**

-   Cyclistic marketing analytics team

From these insights, my team will design a new marketing strategy to convert casual riders into annual members.

My team has to produce a report with the following deliverables:

1\.
A clear statement of the business task

2\.
A description of all data sources used

3\.
Documentation of any cleaning or manipulation of data

4\.
A summary of your analysis

5\.
Supporting visualizations and key findings

6\.
Your top three recommendations based on your analysis

## 2. PREPARE

### What are data sources used?

Documentation of any cleaning or manipulation of data

#### Data location

Lyft Bikes and Scooters, LLC ("Bikeshare") operates the City of Chicago's ("City") Divvy bicycle sharing service.
Bikeshare and the City are committed to supporting bicycling as an alternative transportation option.
As part of that commitment, the City permits Bikeshare to make certain Divvy system data owned by the City ("Data") available to the publicData organization.

We'll use that Data in Case study as Cyclistic's historical trip data.\

The data has been made available by Motivate International Inc. under [this license.](https://www.divvybikes.com/data-license-agreement)
It is a **First-party data.**

#### Data credibility and data bias

The Data itself is a First-party data and it is credible and has no evidence of bias of any kind.

#### Data ethics

There is no any personal information that we can associate with real customers.
Each trip is anonymized.\

We accept all limitations on Data usage noted in "Prohibited conduct" in [**Data License Agreement**.](https://www.divvybikes.com/data-license-agreement)

#### Data tools

At first glance, the overall dataset would be **Large** enough to process and will force any available spreadsheet software to struggle, so our team decided to use R to handle it.\

Let's do that.

Setting the environment.

```{r message=FALSE, warning=FALSE}
library(tidyverse)
library(dplyr)
library(tidyr)
library(janitor)
library(lubridate)
library(ggplot2)
library(plotly)
library(scales)
library(skimr)
library(sf)
options(dplyr.summarise.inform = FALSE)
options(max.print=100)
```

Take a mention on the current working folder in output of `getwd()` and if redefine it if needed:

```{r}
getwd()
# uncomment and redefine it if needed (use your actual folder)
# setwd("../Coursera/Case_study/")
```

Defining the directory where all original zip files will be placed:

```{r}
zip_dir<- paste0(getwd(),"/Divvy_tripdata/") 
```

Defining the directory csv files to extract:

```{r}
csv_Dir<- paste0(getwd(),"/Divvy_tripdata/csv/") 
```

Auto unzipping files to `zip_dir`:

```{r}
files <- list.files(path = zip_dir, pattern = "*.zip")
for (i in files) {
  unzip(paste0(zip_dir,i), exdir=csv_Dir)
}
```

Reading csv files and nesting them into Large list (almost 2Gb).\

Wait a little bit, please.
Need a minute to execute:

```{r}
temp <- list.files(path = csv_Dir, pattern = "*.csv")
myfiles <- lapply(paste0(csv_Dir,temp), read.csv)
```

So we have all the data we need in one place.\

We haven't performed any data manipulations so far.\

Let's go further.

## 3. PROCESS

### Data transformations

#### **Do the data frames have the same columns & types?**

Let's check it out:

```{r}
janitor::compare_df_cols_same(myfiles)
```

TRUE - means that all columns in all data frames have appropriate names and types of data.\

The alternative method is to print out any mismatches:

```{r}
janitor::compare_df_cols(myfiles, return = "mismatch")
```

Returned 0 rows -\> all data frames are ready to bind.

#### Finally forming united table.

Binding data frames by row, making a longer result (few seconds to execute, don't panic):

```{r binding to raw_df}
raw_df <- dplyr::bind_rows(myfiles)
```

Let's look in:

```{r}
head(raw_df)
```

#### We need to convert date related columns to appropriate type:

```{r casting dates}
raw_df$started_at = ymd_hms(raw_df$started_at) 
raw_df$ended_at = ymd_hms(raw_df$ended_at) 
```

#### Adding a calculated columns

It is obvious that we'll need a duration information in our analysis.\

Let's add a calculated column `trip_duration` that counts trip duration in **minutes**.

```{r adding trip duration (in minutes)}
raw_df[,"trip_duration"] <- as.numeric(as.duration(raw_df$ended_at - raw_df$started_at), "minutes")
```

Let's evaluate values domain for integrity:

```{r glimpse at raw_df }
skim_without_charts(raw_df)
```

### Data integrity

Tables naming

+----------------------------+---------------------------+------------------+
| Tables used in the project | Table purpose             | Table dimensions |
+============================+===========================+==================+
| raw_df                     | untouched imported data   | 5,858,018 x 14   |
+----------------------------+---------------------------+------------------+
| trip_df                    | store filtered valid data | 5,548,446 x 15   |
+----------------------------+---------------------------+------------------+
| stations_df                | station dictionary        | 1,716 x 6        |
+----------------------------+---------------------------+------------------+
| stations_df2               | temporary table           |                  |
+----------------------------+---------------------------+------------------+
| df                         | tibble, cleaned data      | 5,548,446 x 16   |
+----------------------------+---------------------------+------------------+

1.  **Domain integrity:** Domain integrity ensures that each value in a column falls within the permissible range of the domain of that column.
    Moreover, the conditions for default and null values must also be met.

2.  **Entity integrity:** Entity integrity ensures that each row of the database has a non-null unique primary key.

3.  **Referential integrity:** Referential integrity ensures a valid relationship between two tables by checking the relationship between the foreign key and primary key in those tables.

Data set consists of 5.858.018 observations with 14 characteristics (columns).\

Time scope of all trips is relevant to the scope of business problem.

Quantitative (numeric and POSIXct) data:

+---------------+-----------------------+-----------------------+--------------+------------------+----------------------+--------------------------------+
| Attribute     | min value             | max value             | number of NA | Domain integrity | **Entity integrity** | Notes                          |
+===============+=======================+=======================+==============+==================+======================+================================+
| started_at    | `2022-01-01 00:00:05` | `2023-01-31 23:56:09` | 0            | \+               | \+                   |                                |
+---------------+-----------------------+-----------------------+--------------+------------------+----------------------+--------------------------------+
| ended_at      | `2022-01-01 00:01:48` | `2023-02-04 04:27:03` | 0            | \+               | \+                   |                                |
+---------------+-----------------------+-----------------------+--------------+------------------+----------------------+--------------------------------+
| trip_duration | -10353.35             | 41387.25              | 0            | fault            | \+                   | negatives, very high `std dev` |
+---------------+-----------------------+-----------------------+--------------+------------------+----------------------+--------------------------------+
| start_lat     | 41.64                 | 45.63503              | 0            | fault            | \+                   | too big range for a city       |
+---------------+-----------------------+-----------------------+--------------+------------------+----------------------+--------------------------------+
| start_lng     | -87.8                 | -73.796               | 0            | fault            | \+                   | too big range for a city       |
+---------------+-----------------------+-----------------------+--------------+------------------+----------------------+--------------------------------+
| end_lat       | 0.00                  | 42.37000              | 5985         | fault            | fault                | zeros                          |
+---------------+-----------------------+-----------------------+--------------+------------------+----------------------+--------------------------------+
| end_lng       | -88.1                 | 0.00000               | 5985         | fault            | fault                | zeros                          |
+---------------+-----------------------+-----------------------+--------------+------------------+----------------------+--------------------------------+

: Qualitative (character) data:

+--------------------+-------------------------+------------------+------------------+----------------------+------------------------------------------------+
| Attribute          | Number of empty entries | Number of unique | Domain integrity | **Entity integrity** | Notes                                          |
+====================+=========================+==================+==================+======================+================================================+
| ride_id            | 0                       | 5858018          | \+               | \+                   | \+                                             |
+--------------------+-------------------------+------------------+------------------+----------------------+------------------------------------------------+
| rideable_type      | 0                       | 3                | \+               | \+                   | \+                                             |
+--------------------+-------------------------+------------------+------------------+----------------------+------------------------------------------------+
| start_station_name | 859.785                 | 1682             | \+               | fault                | number of stations is greater than station IDs |
+--------------------+-------------------------+------------------+------------------+----------------------+------------------------------------------------+
| start_station_id   | 859.785                 | 1314             | \+               | \+                   | \+                                             |
+--------------------+-------------------------+------------------+------------------+----------------------+------------------------------------------------+
| end_station_name   | 920.582                 | 1700             | \+               | fault                | number of stations is greater than station IDs |
+--------------------+-------------------------+------------------+------------------+----------------------+------------------------------------------------+
| end_station_id     | 920.582                 | 1319             | \+               | \+                   | \+                                             |
+--------------------+-------------------------+------------------+------------------+----------------------+------------------------------------------------+
| member_casual      | 0                       | 2                | \+               | \+                   | \+                                             |
+--------------------+-------------------------+------------------+------------------+----------------------+------------------------------------------------+

How many observations are affected with empty start and finish points?

```{r Rows with missing data}
raw_df %>% 
  filter(start_station_name == "" | end_station_name == ""  ) %>% 
  nrow()
```

\

### Data issues

Most of the lost of geo data fell on electric bikes.

```{r Missing stations name}
raw_df %>% 
  filter(start_station_name == "" |
           end_station_name == "" ) %>%
  group_by(rideable_type) %>% 
  summarise(sum= n()) %>%
  ggplot(aes(rideable_type, y= sum )) +
  geom_col(aes(fill= rideable_type), show.legend = FALSE) +
  scale_y_continuous(labels = label_comma()) +
  geom_label(aes(y = sum, x = rideable_type, label = sum,
               color = rideable_type), hjust =0.8, vjust = 0, show.legend = FALSE)+
  labs(title = "Trips with missing station's names by type of bike",
       caption = "Data for the whole year",
       x ="", y= "",
       fill='Type of rider')+
  theme(axis.text.y=element_blank(),
        axis.text = element_text(size = 16) ) 
```

The reason could be a complete discharge of the batteries.

100 observations have negative trip duration.

```{r Negative trip durations}
raw_df %>%
  filter(trip_duration < 0) %>%
  group_by(rideable_type) %>%
  summarise(sum= n()) 

```

**The data is not clean:**

1.  5985 trips (0.1% of all trips) have no gps data at all.
    This concern we might take into account if we'll plan to investigate routes.
    Some observations at end stations include zeros.

2.  Station's naming is not consistent.
    Number of station,s ID is less than station's names.

3.  1.340.374 (23 % of all trips) of data concerning station's names is empty.
    99,5 % among them are electric bikes.

4.  Some `started_at` is greater than `ended_at`.
    It means negative `trip_duration` .

5.  Standard deviation of `trip_duration` is unreasonably high: ( 175 min, while mean =19 min).
    This clearly indicates the presence of outliers.

**Solutions:**

1.  Exclude data with negative `trip_duration` (100 observations)

2.  Exclude data with too big `trip_duration` (greater than 1499 minutes) and with simultaneously empty `end_station_name` .

3.  Exclude classic bike's trips with missing station's names (3863 observations)

4.  Restore stations ID by geo data where possible.
    Maybe this won't screw the overall patterns but it's better to restore the missing data.

Visual checking the data distribution of trips with missing station's names:

#### Dropping irrelevant data

The data has to be processed.
We will remove:

-   trips that are taken by staff as they service and inspect the system

-   any trips that were below 60 seconds in length (potentially false starts or users trying to re-dock a bike to ensure it was secure).

Let's take a look on distribution of "strange" observations\
where `trip_duration > 1499 & rideable_type != "docked_bike"`

```{r Dirty data distribution, message=FALSE, warning=FALSE}
raw_df %>% 
  # filter(start_station_name == "" | end_station_name == ""  ) %>% 
  filter(rideable_type != "docked_bike") %>%
  filter(trip_duration > 0) %>% 
  # nrow()
  arrange(trip_duration) %>% 

  ggplot() +

  geom_density(aes(x=trip_duration)) +
  scale_y_log10()
```

The surge at mark 25h(1500 minutes) might be a service notation, e.g. bikes that had been left out of parking stations, stolen bikes or defected.

Finally, we'll consider trip a **valid** if it meets following conditions:

1.  `rideable_type != "docked_bike"` to exclude service observations.

2.  `trip_duration` \> 1 minute (exclude false starts and re-docking).

3.  `trip_duration` \< 1499 minutes(25 hours).
    ([NOTE: If you do not return a bike within a 24-hour period, you may be charged a lost or stolen bike fee of \$250 (plus tax](https://help.divvybikes.com/hc/en-us/articles/360033484791-What-if-I-keep-a-bike-out-too-long-)).

4.  `end_station_name` is not empty

```{r}
raw_df %>% 
  filter(rideable_type == "docked_bike" | is.na(end_lat) |
           trip_duration < 1 | trip_duration > 1499 ) %>% 
  nrow()
```

This assumption will exclude 308,332 (5.2 %) observations from "dirty" data.
This is acceptable.

We keep the source untouched and put valid data to `trip_df` :

```{r trip_df}
trip_df <- raw_df %>% 
  filter(rideable_type != "docked_bike" & !is.na(end_lat)) %>% 
  filter(trip_duration > 1 &
           trip_duration < 1499 ) 
```

### Logic behind the following restoring missed station names:

\
1.
Build a data frame (`stations_df`) with all station's names and appropriate geo data\
2.
Restore station's names according to the dictionary geo data.

Let's try.

#### **Station's names.**

To avoid duplicates we round the geo data to 4 decimals places (11 m accuracy).

```{r stations_df}
# setting geo data accuracy (decimal places)
geo_acc <- 4

# parcing end stations
stations_df <- trip_df %>% 
    filter(end_station_name != "" & 
             (end_lat != 0 | !is.na(end_lat) ) ) %>% 
  group_by(end_station_name) %>% 

  # we use means here to increase geo data accuracy of stations
  summarise(latit = mean(end_lat), 
            lngit = mean(end_lng)) %>% 
  unique()

# all columns with postfix '2' at the end will serve later as joining instances 
stations_df[,"end_lat2"] = round(stations_df$latit,geo_acc)
stations_df[,"end_lng2"] = round(stations_df$lngit,geo_acc)

# adding station IDs
stations_df <-
  left_join(stations_df, trip_df, by = c("end_station_name"), multiple = "first") %>% 
  select("end_station_id",
         "end_station_name",
         "latit",
         "lngit",
         "end_lat2",
         "end_lng2")
# renaming
stations_df <- 
  rename(stations_df, all_of( c(station_name = "end_station_name", 
                                station_id =  "end_station_id")) )


# parcing start stations
# stations_df2 - start_station data frame
stations_df2 <- trip_df %>% 
    filter(start_station_name != "" ) %>% 
  group_by(start_station_name) %>% 
  # all columns with '2' at the end will serve later as joining instances 
  # we use means here to increase geo data accuracy
  summarise(latit = mean(start_lat), 
            lngit = mean(start_lng) ) %>% 
  unique()

# all columns with postfix '2' at the end will serve later as joining instances 
stations_df2[,"end_lat2"] = round(stations_df2$latit,geo_acc)
stations_df2[,"end_lng2"] = round(stations_df2$lngit,geo_acc)


# adding station IDs
stations_df2 <-
  left_join(stations_df2, trip_df, by = c("start_station_name"), multiple = "first") %>% 
  select("start_station_id",
         "start_station_name",
         "latit",
         "lngit",         
         "end_lat2",
         "end_lng2")
# renaming
stations_df2 <- 
  rename(stations_df2, all_of( c(station_name = "start_station_name", 
                                station_id =  "start_station_id")) )

stations_df <-
  bind_rows(stations_df, stations_df2) %>% 
  dplyr::distinct(station_name, .keep_all = TRUE) 

```

```{r}
# geo_df <-
# stations_df %>% 
  # filter(end_lng2 < -87)
```

```{r}

# chi_map <- read_sf("https://raw.githubusercontent.com/thisisdaryn/data/master/geo/chicago/Comm_Areas.geojson")

  # ggplot(chi_map) +
  # geom_sf() + 
  # geom_point(data = geo_df, mapping = aes(x = end_lng2, y = end_lat2),
  #            size = 1, stroke = 0, color = "red") +
  # geom_text(data = geo_df, aes(x=end_lng2, y=end_lat2, label=station_name),
  #           size = 3,  check_overlap = TRUE, color= "blue") # hjust=0, vjust=-1,
```

```{r}
# insp_plt <- ggplot(data = chi_map) + 
#   geom_sf() +
#   geom_point(data = geo_df, 
#              aes(x = end_lng2, y = end_lat2, 
#                  colour = "red"))  # , label=station_name
# ggplotly(insp_plt)
```

#### **Restoring station's names.**

##### Restoring `end_station_name` and `end_station_id`.

```{r End stations}
# service columns with rounded geo data
trip_df[,"end_lat2"] <- round(trip_df$end_lat,geo_acc)
trip_df[,"end_lng2"] <- round(trip_df$end_lng,geo_acc)
trip_df[,"restored"] <- NA

trip_df <- 
  left_join(trip_df, stations_df, by = c("end_lat2","end_lng2"), multiple = 'first') 

# logging restoration
trip_df$restored = 
  ifelse(trip_df$end_station_name == "" & !is.na(trip_df$station_name),
         "end_station_name",
         ifelse(trip_df$end_station_id == "" & !is.na(trip_df$station_id),
                "end_station_id", NA)
  )
# adding end_station_name
trip_df$end_station_name = 
  ifelse(trip_df$end_station_name == ""& !is.na(trip_df$station_name),
         trip_df$station_name,
         trip_df$end_station_name) 
# adding end_station_id
trip_df$end_station_id = 
  ifelse(trip_df$end_station_id == ""& !is.na(trip_df$station_id),
         trip_df$station_id,
         trip_df$end_station_id) 

# dropping service joining columns
trip_df <- within(trip_df, rm("end_lat2",
                                        "end_lng2",
                                        "station_id",
                                        "station_name" ))
```

##### Restoring `start_station_name` and `start_station_id`.

```{r Start stations}
trip_df[,"start_lat2"] <- round(trip_df$start_lat,geo_acc)
trip_df[,"start_lng2"] <- round(trip_df$start_lng,geo_acc)

stations_df <- 
  rename(stations_df, all_of( c(start_lat2 = "end_lat2", 
                                start_lng2 =  "end_lng2")) )

trip_df <- 
  left_join(trip_df, stations_df, by = c("start_lat2","start_lng2"), multiple = 'first') 
# logging restoration
trip_df$restored = 
  ifelse(trip_df$start_station_name == "" & !is.na(trip_df$station_name),
         "start_station_name",
         ifelse(trip_df$start_station_id == "" & !is.na(trip_df$station_id),
                "start_station_id", NA)
  )

# adding start_station_name
trip_df$start_station_name = 
  ifelse(trip_df$start_station_name == "" & !is.na(trip_df$station_name),
         trip_df$station_name,
         trip_df$start_station_name) 
# adding start_station_id
trip_df$start_station_id = 
  ifelse(trip_df$start_station_id == "" & !is.na(trip_df$station_id),
         trip_df$station_id,
         trip_df$start_station_id) 

# dropping joining columns
trip_df <- within(trip_df, rm(
                                  "start_lat2",
                                  "start_lng2",
                                  "station_id",
                                  "station_name",
                                  "latit.x",
                                  "lngit.x",
                                  "latit.y",
                                  "lngit.y"))
nrow(filter(trip_df, !is.na(restored) ) )
```

We've managed to restore station's names within 312.438 observations.\

Converting cleaned data frame to tibble:

```{r Converting cleaned data frame to tibble}
df <- as_tibble(trip_df) 
```

Adding a trip the day of the week :

```{r Adding the day of the week }
df[, "weekday"] <- wday(df$started_at, label = TRUE)
```

## 4. ANALYSE

#### Assumptions and constraints

Let's take a look at distribution of trips over data set:

```{r distribution of trips over data set, warning=FALSE}
df %>% 
  ggplot(aes(x=trip_duration, y= member_casual)) +
  geom_boxplot( outlier.colour = "red",
                outlier.stroke = 0, 
                outlier.alpha = 0.1,
                varwidth = FALSE) +
  scale_x_log10()
```

There are too many outliers that might skew overall statistics.\

We constrain data with :

-   the upper limit to `mean + 5sigma` = 142 minutes

```{r Upper trip limit}
(trip_limit <- mean(df$trip_duration) + 5* sd(df$trip_duration) )
```

It removes only 17.837 rows (0.3 % of all trips).
So our conclusions will be based on 99.7 % of "clean" data.

```{r count dropped rows}
count(filter(df, trip_duration > trip_limit))
```

### Descriptive statistics

#### Number of trips throughout a year

```{r Number of trips throughout a year}
df %>% 
  group_by(member_casual, rideable_type) %>% 
  summarise(ride_count = n()) %>%
  
  ggplot() + 
  geom_col(aes(x = member_casual, y = ride_count,  fill = rideable_type ),
           show.legend = TRUE) + 
  geom_label(aes(x = member_casual, y = ride_count,  
                 label = format(ride_count,big.mark=",") , color = rideable_type),
             position = position_stack(vjust = 0.6) , show.legend = FALSE) + 
  labs(title = "Number of trips per year", 
       caption = "Data for the whole year",
       x ="", y= "") +  
  scale_y_continuous(labels = label_comma()) +
  theme(
    # axis.text.y=element_blank(),
        axis.text = element_text(size = 16)
        ) 
```

**Insights:**

-   **Casual** riders **prefer e-bikes** (+42.8% over classic bikes)

-   while Cyclistic's **members** choose e-bikes and classic bikes roughly equally.

-   **Members** use Cyclistics's services **much more often** than casual riders (+ 60 %).

+----------------+---------------------+--------------------+---------------+
|                | casual riders       | members            | total         |
+================+=====================+====================+===============+
| classic bikes  | 889.658             | 1.758.197          | 2.647.855     |
+----------------+---------------------+--------------------+---------------+
| electric bikes | 1.242.534 (+42.8 %) | 1.658.057 (-4.3 %) | 2.900.591     |
+----------------+---------------------+--------------------+---------------+
| total bikes    | 2.132.193           | 3.416.254          | 5.548.446     |
+----------------+---------------------+--------------------+---------------+

```{r Number of trips}
df %>% 
  group_by(rideable_type, member_casual) %>%
  summarise(ride_count = n()) 
```

#### Average duration of one trip throughout a year

```{r Average duration of one trip, warning=TRUE}
df %>% 
  group_by(member_casual, weekday) %>% 
  summarise(ride_mean = round(mean(trip_duration),1))  %>% 
  # summarise(ride_mean = round(mean(trip_duration),1))  %>%   
  ggplot() +
  geom_col(aes(y = ride_mean, x = weekday, fill = member_casual ),
           position = "dodge") + #, show.legend = FALSE, , alpha = 0.8
  labs(title = "Average duration of one trip by day of the week (minutes)",
       caption = "Data for the whole year",
       x ="", y= "minutes",
       fill='Type of rider') +
  scale_y_continuous(labels = label_comma()) +
geom_label(aes(y = ride_mean, x = weekday, label = ride_mean,
               color = member_casual), hjust =0.8, vjust = 1.5, show.legend = FALSE)

```

```{r the average ride_length for members and casual riders}
df %>% 
  group_by(member_casual) %>% 
  summarise(ride_mean = round(mean(trip_duration),1)) 
```

Insights:

-   Average duration of **casual riders is significantly higher** (+ 51 %)

-   Trips on **Wednesdays** are 17 % shorter than on weekends among all customers.

#### The maximum ride duration

Finding the maximum ride duration we assume:

1.  Bike is not docked

2.  Start and end stations are defined

3.  We'll look up through the source data

```{r The maximum ride duration}
raw_df  %>% 
  filter(rideable_type != "docked_bike") %>%
  filter(start_station_name !="" & end_station_name != "") %>% 
  group_by(rideable_type) %>% 
  summarise(max=as.duration(max(ended_at - started_at))) 
```

```{r maximum ride duration}
raw_df  %>% 
  filter(rideable_type != "docked_bike") %>%
  filter(start_station_name !="" & end_station_name != "" &
           !is.na(end_lat)) %>% 
  group_by(rideable_type) %>% 
  summarise(max=as.duration(max(ended_at - started_at))) %>%

  ggplot() + 
  geom_col(aes(y = max, x = rideable_type, fill = rideable_type ),
           show.legend = FALSE) +
  geom_label(aes(y = max, x = rideable_type, 
                 label = format(max,big.mark=",") )
             , vjust = 2) + 
  labs(title = "The maximum ride duration", 
       caption = "Data for the whole year",
       x ="", y= "") +  
  scale_y_continuous(labels = label_comma()) +
  theme(axis.text.y=element_blank(),
        axis.text = element_text(size = 16)
        ) 
```

```{r long rides on e-bikes, message=FALSE}
raw_df %>% 
  filter(rideable_type == "electric_bike") %>%
  filter(start_station_name !="" & end_station_name != "" &
           !is.na(end_lat)) %>% 
  select("rideable_type", "member_casual", "trip_duration") %>% 
  arrange(desc(trip_duration))
```

Insights:

-   Classic bike still leads the way in 1 full day trips

-   The electric bike was able to last for 8 hours)

-   The absolute majority of long rides on e-bikes were taken by members.
    (Pricing starts at \$1 to unlock plus \$0.39/minute for casual riders (\$0 to unlock plus **\$0.16/minute for members)**.)

#### The mode of day of week throughout a year

```{r overall mode of day of week}
Mode <- function(x) {
  ux <- unique(x)
  ux[which.max(tabulate(match(x, ux)))]
}

num_weeks <- as.numeric(max(df$ended_at) - min(df$ended_at),"weeks") 
df %>% 
  group_by(member_casual, weekday) %>%
  summarise(mode = Mode(weekday), 
            count = n() / num_weeks )  %>% 
  # arrange(desc(count))
  
  ggplot() +
  geom_col(aes(y = count, x = weekday, fill = member_casual ),
           position = "stack") + #, show.legend = FALSE, , alpha = 0.8
  labs(title = "Number of trips by day of the week",
       caption = "Data for the whole year",
       x ="", y= "Count",
       fill='Type of rider') +
  scale_y_continuous(labels = label_comma()) +
  geom_label(aes(x = weekday, y = count,  label = round(count), color = member_casual), 
      hjust =0.5, vjust = 1.7, show.legend = FALSE, size = 3,
      position = "stack") # position_dodge(width = 1)
```

```{r mode of dow by member_casual}
Mode <- function(x) {
  ux <- unique(x)
  ux[which.max(tabulate(match(x, ux)))]
}

num_weeks <- as.numeric(max(df$ended_at) - min(df$ended_at),"weeks") 
df %>% 
  group_by(member_casual, weekday) %>%
  summarise(mode = Mode(weekday), 
            count = n() / num_weeks )  %>% 
  # arrange(desc(count))
  
  ggplot() +
  geom_col(aes(y = count, x = weekday, fill = member_casual ),
           position = "dodge") + #, show.legend = FALSE, , alpha = 0.8
  labs(title = "Number of trips by day of the week",
       caption = "Data for the whole year",
       x ="", y= "Count",
       fill='Type of rider') +
  scale_y_continuous(labels = label_comma()) +
  geom_label(aes(x = weekday, y = count,  label = round(count), color = member_casual), 
      hjust =0.5, vjust = 1.7, show.legend = FALSE, size = 3,
      position = position_dodge(width = 1)) 
```

+---------------+-------------------------+---------------------------+--------------------------------------+
| Sample        | The mode                | Number of trips           | Anti-mode (the least frequent score) |
+===============+=========================+===========================+======================================+
| Member riders | Thursday\               | 9,584 (7,901 at Saturday) | Sunday - 6,955                       |
|               | (Tue-Thu roughly equal) |                           |                                      |
+---------------+-------------------------+---------------------------+--------------------------------------+
| Casual riders | Saturday                | 7,542                     | Tuesday- 4,362                       |
+---------------+-------------------------+---------------------------+--------------------------------------+
| Population    | Saturday                | 15,443                    | Monday- 13,071                       |
+---------------+-------------------------+---------------------------+--------------------------------------+

Insights:

-   **Monday** is the least busy day of the week (anti-mode).

-   **Saturday** is the most busy day at Cyclistic's mainly thanks to casual riders.

```{r Mode of day of week}

# num_weeks <- as.numeric(max(df$ended_at) - min(df$ended_at),"weeks") 
# df %>% 
#   group_by( weekday) %>%
#   summarise(mode = Mode(weekday), 
#             count = n() / num_weeks ) %>% 
#   arrange(desc(count))
```

#### Different seasons to make some initial observations

```{r overall year load}
df %>% 
  filter(started_at < ymd("2023-01-01")) %>% # limit to a calendar year
  group_by(member_casual, Month=  month(started_at, label = TRUE)) %>%
  summarise(count = n()) %>%

  ggplot(aes(x = Month, y= count, fill = member_casual)) +
  geom_col(position = "dodge") +
  labs(title = "Trips by month",
       caption = "",
       x ="", y= "Count of trips",
       fill='Type of rider') +
  scale_y_continuous(labels = label_comma()) 
# +
#   geom_text(aes(x = Month, y= count, label = count),
#       hjust =0.5, vjust = 1.7, show.legend = FALSE, size = 2,
#       position = position_dodge(width = 1))

```

```{r}
df %>% 
  filter(started_at < ymd("2023-01-01")) %>% # limit to a calendar year
  group_by(member_casual, Month=  month(started_at, label = TRUE)) %>%
  summarise(count = n()) %>%

  ggplot(aes(x = Month, y= count, fill = member_casual)) +
  geom_col(position = "stack") +
  labs(title = "Trips by month",
       caption = "",
       x ="", y= "Count of trips",
       fill='Type of rider') +
  scale_y_continuous(labels = label_comma()) 
# +
#   geom_text(aes(x = Month, y= count, label = count),
#       hjust =0.5, vjust = 1.7, show.legend = FALSE, size = 2,
#       position = position_dodge(width = 1))
```

Insights:

-   **Jan and Feb are the toughest month at Cyclistics**.
    The income is only \~ 16% of year peaks.
    \
    **Members generated \~80% of income during Jan and Feb**.

-   In the summer months, the proportion begins to level off.

```{r}
df %>% 
  # filter(started_at < ymd("2023-01-01")) %>% # limit to a calendar year
  # group_by( Month=  month(started_at, label = TRUE), member_casual) %>%
  # reframe(count = n()) 
```

Average trip duration by day of week, month, rider

```{r}
# insp_plt <-
  df %>%
  ggplot() +
  geom_bar(aes(x = month(started_at), fill = member_casual),
           position = "dodge", alpha = 1) +
  labs(title = "Trips by days of week",
       caption = "",
       x ="", y= "Count of trips",
       fill='Type of rider') +
  scale_y_continuous(labels = label_comma())

# ggplotly(insp_plt)
```

```{r message=FALSE, warning=FALSE}
df %>% 
  ggplot(aes(x = trip_duration, fill = member_casual)) +
  geom_histogram(binwidth=5,position = "dodge",alpha = 1) + #
  labs(title = "Trips by duration of ride",
       caption = "Bin duration: 5 min",
       x ="Minutes", y= "Count of trips",
       fill='Type of rider') +
  scale_y_continuous(labels = label_comma())+
  xlim(0, 65)
```

```{r}
# df %>% 
#   arrange(desc(trip_duration))
  # nrow()
# as.numeric(ride_length,"minutes")

df %>% 
  summary()
```

```{r}
df["rideable_type"] %>%
  n_distinct()
```
